---
title: 다양한 자연어 처리 기술
categories:
  - AI
  - Transfomer
  - 자연어
tags:
  - GPT
toc: true
toc_sticky: true
toc_label: ""
toc_icon: ""
comments: true
---

짧다면 짧고 길다면 긴 1년이 지났습니다. 여러 신기술을 보던 중 관심이 생긴 AI에 대해서 찍먹 해보려고 적어보는 포스팅입니다. 🥹<br>
한글부터도 너무나 어려운데 언어는 얼마나 어려울지.. 


---

# 다양한 자연어 처리 기술

1. 자연어 생성 모델:
   1. 자연어 생성 모델은 입력 데이터를 바탕으로 인간이 이해할 수 있는 자연어 텍스트를 생성하는 모델
   2. GPT와 같은 Transformer 기반 모델이 대표적인 자연어 생성 모델이며, 문장 생성, 요약, 번역 등의 태스크에 활용
   3. 최근에는 sLLM을 활용한 자연어 생성 모델이 더욱 발전하고 있으며, 인간과의 대화, 콘텐츠 생성 등 다양한 응용 분야에 적용
2. Transformer 모델:
   1. GPT(Generative Pre-trained Transformer)와 BERT(Bidirectional Encoder Representations from Transformers)는 대표적인 Transformer 모델
   2. Transformer 모델은 기존 RNN(Recurrent Neural Network) 기반 모델들과 달리 Attention 메커니즘을 사용하여 입력 간 상호 관계를 모델링 함
   3. GPT는 언어 생성 태스크에 강점을 보이며, BERT는 다양한 자연어 이해 태스크에서 뛰어난 성능을 보임
<details>
<summary>Attention 메커니즘</summary>
<div markdown="1">

    1. 가중치 계산
      - 입력 데이터의 각 부분(=토큰)과 현재 처리 중인 토큰의 관련성을 계산
      - 이를 통해 현재 처리중인 부분에 더 관련성이 높은 입력 부분에 더 큰 가중치 부여
    2. 동적 가중치 할당 (Ex.  ****기계 번역 task에서 Attention 메커니즘은 현재 번역할 단어와 입력 문장의 각 단어 간의 관련성을 동적으로 계산하여 가중치를 할당. 이를 통해 문맥에 맞는 번역을 할 수 있게 됨)
    3. 입력 데이터의 문맥적 정보를 효과적으로 활용가능
    4. 병렬 처리
      - 입력 데이터의 모든 부분을 병렬로 처리 할 수 있어 효율적
      - 이는 RNN과 달리 순차적 처리가 필요 없기 때문
    5. 장기 의존성 모델링
      - 입력 데이터의 장기 의존성을 효과적으로 모델링 가능
      - 이를 통해 문맥 정보를 더 잘 활용할 수 있어 성능 향상에 기여함
</div>
</details>
<details>
<summary>RNN</summary>
<div markdown="1">

     순환 신경망의 약자로, 시계열 데이터 처리에 특화된 인공신경망 모델
     RNN은 언어 모델링, 기계 번역, 음성 인식 등 다양한 자연어 처리 분야에서 널리 사용되었으나, 장기 의존성 문제(Long-Term Dependency Problem)를 겪는 한계가 있음
     이를 극복하기 위해 등장한 것이 Transformer 모델. Transformer는 Attention 메커니즘을 사용하여 입력 간 상호 관계를 모델링하며, RNN에 비해 병렬 처리가 용이하다는 장점이 있음

    - RNN 특징:
      1. 순환 구조: RNN은 순환 구조를 가지고 있어, 이전 시점의 출력이 다음 시점의 입력으로 사용. 이를 통해 시계열 데이터의 특성을 모델링 가능
      2. 메모리 기능: RNN은 이전 시점의 정보를 메모리로 저장하고, 이를 활용하여 현재 시점의 출력을 생성. 이를 통해 시계열 데이터의 맥락 정보를 반영 가능
      3. 가변 길이 입력/출력: RNN은 입력과 출력의 길이가 가변적일 수 있어, 다양한 길이의 시계열 데이터를 처리가능

</div>
</details><br>

   
3. VectorDataBase와 RAG(Retrieval-Augmented Generation) :
   1. VectorDataBase는 임베딩 벡터를 저장하고 효율적으로 검색 할 수 있는 데이터 베이스 시스템
   2. RAG는 Transformer 모델과 VectorDataBase를 결하바여 지식 검색과 생성을 동시에 수행하는 모델
   3. RAG는 질문에 대한 답변을 생성할때 VectorDataBase에서 관련 정보를 검색하여 활용
4. sLLM(Scaled Language Models):
   1. sLLM은 최근 등장한 대규모 언어 모델(LLM) 기술로, GPT-3와 같은 기존 LLM 모델보다 더 큰 규모의 모델을 의미
   2. sLLM은 수십억 개의 파라미터와 수조 개의 토큰으로 학습되어 더욱 강력한 성능을 가짐
   3. sLLM은 자연어 생성, 질의응답, 추론 등 다양한 태스크에서 뛰어난 성능을 보이고 있음

Etc.

- 시계열(Time Series)
  - 시계열 데이터는 시간의 흐름에 따라 순차적으로 관찰된 데이터를 의미
  - 예를 들어, 주식 가격, 온도, 매출 데이터 등이 시간에 따라 기록된 데이터가 시계열 데이터
    시계열 데이터는 과거 정보가 현재와 미래에 영향을 미치는 특성을 가지고 있음
- 신경망(Neural Network)
  - 신경망은 인간의 뇌 구조를 모방하여 만든 기계학습 모델
  - 신경망은 입력층, 은닉층, 출력층으로 구성되어 있으며, 각 층의 노드들이 가중치와 편향 값을 가지고 연결되어 있고 신경망은 데이터로부터 자동으로 특징을 학습하고, 복잡한 패턴을 모델링할 수 있는 장점이 있음
    대표적인 신경망 모델로는 퍼셉트론, 다층 퍼셉트론, 합성곱 신경망(CNN), 순환 신경망(RNN) 등이 있음
- LLM(Large Language Model)과 sLLM(Smaller Language Model)의 주요한 차이점
  1. 모델 크기: LLM은 수십억 개 이상의 매개변수를 가지는 거대한 규모의 언어 모델, sLLM은 LLM에 비해 매개변수의 수가 훨씬 작은 규모의 언어 모델
  2. 성능: LLM은 sLLM에 비해 일반적으로 더 높은 성능을더 가짐
     큰 데이터 세트와 컴퓨팅 자원을 활용하여 훨씬 복잡한 작업을 수행할 수 있음
     sLLM은 LLM에 비해 성능이 다소 낮지만, 특정 작업에 대해서는 LLM과 유사한 수준의 성능을 보일 수 있음
  3. 배포 및 활용: LLM은 대규모 데이터 센터에 배포되어 클라우드 기반의 API 형태로 제공되는 경우가 많음 .sLLM은 상대적으로 작은 규모이므로 개인 컴퓨터나 모바일 기기에서 로컬로 실행될 수 있음.
  4. 사용 사례: LLM은 일반적인 언어 이해 및 생성 작업에 주로 사용(대화형 챗봇, 번역, 요약 등의 작업에 활용) sLLM은 특정 도메인이나 응용 프로그램에 맞춰 최적화되어 사용됨, 예를 들어 의료 분야의 전문 지식 질의응답 등에 활용

  요약하면, LLM은 더 큰 규모와 성능을 가지지만 배포와 활용에 제약이 있는 반면, sLLM은 상대적으로 작은 규모와 성능을 가지지만 더 다양한 활용 환경에서 사용가능